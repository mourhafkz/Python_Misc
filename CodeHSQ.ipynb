{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from sklearn import metrics, naive_bayes, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble, model_selection\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "stop_words_en = stopwords.words('english')\n",
    "stop_words_es = stopwords.words('spanish')\n",
    "\n",
    "import spacy\n",
    "nlpEN = spacy.load('en_core_web_sm')\n",
    "# nlpES = spacy.load('es_core_news_sm')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading the data   --- these functions are taken and modified from ashraf2019\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def iter_docs(author):\n",
    "    author_attr = author.attrib\n",
    "    doc_dict = author_attr.copy()\n",
    "    doc_dict['text'] = [' '.join([doc.text for doc in author.iter('document')])]\n",
    "    return doc_dict\n",
    "\n",
    "\n",
    "def create_data_frame(input_folder):\n",
    "    os.chdir(input_folder)\n",
    "    all_xml_files = glob.glob(\"*.xml\")\n",
    "    truth_data = pd.read_csv('truth.txt', sep=':::', names=['author_id', 'author'], engine=\"python\")\n",
    "    temp_list_of_DataFrames = []\n",
    "    text_Data = pd.DataFrame()\n",
    "    for file in all_xml_files:\n",
    "        etree = ET.parse(file)  # create an ElementTree object\n",
    "        doc_df = pd.DataFrame(iter_docs(etree.getroot()))\n",
    "        doc_df['author_id'] = file[:-4]\n",
    "        temp_list_of_DataFrames.append(doc_df)\n",
    "    text_Data = pd.concat(temp_list_of_DataFrames, axis=0)\n",
    "\n",
    "    data = text_Data.merge(truth_data, on='author_id')\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English data size 200\n"
     ]
    }
   ],
   "source": [
    "# English Training Dataset\n",
    "en_data = create_data_frame(\"C:/Users/VivAndMourhaf/PycharmProjects/HelloWorld/data/pan21/en\")\n",
    "print(\"English data size\", len(en_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = en_data['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not needed, use class?\n",
    "# y = en_data['author']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loading bad words\n",
    "file = open('D:/data/bad_words.txt', encoding='utf-8')\n",
    "bad_words_array=[]\n",
    "for line in file:\n",
    "    values = line.split(\"\\r\")\n",
    "    word = values[0].strip()\n",
    "    bad_words_array.append(word)\n",
    "file.close()\n",
    "\n",
    "file = open('D:/data/hate_words.txt', encoding='utf-8')\n",
    "hate_words_array=[]\n",
    "for line in file:\n",
    "    values = line.split(\"\\r\")\n",
    "    word = values[0].strip()\n",
    "    hate_words_array.append(word)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hate_words_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_(data):\n",
    "    corpus = []\n",
    "    for tweets in data:\n",
    "        tweets_lowered = tweets.lower()\n",
    "\n",
    "        # Further tweet sanitation\n",
    "#         tweet = re.sub(r'\\s+[a-z]\\s+', ' ', tweets_lowered) # remove single characters like i and a\n",
    "#         tweet = re.sub(r'^[a-z]\\s+', ' ', tweet)  # remove single characters at the beginning like i and a\n",
    "#         tweet = re.sub(r'\\srt\\s+', '', tweet)  # remove extra spaces\n",
    "#         tweet = re.sub(r'#user#', ' #user# ', tweet)  # remove extra spaces\n",
    "#         tweet = re.sub(r'#url#', 'url', tweets_lowered)  # remove extra spaces\n",
    "#         tweet = re.sub('[^A-Za-z0-9]+', '', tweet)  # remove extra spaces\n",
    "#         tweet = re.sub(r'\\s+', ' ', tweet)  # remove extra spaces\n",
    "#         tokenizedTweet = nlpEN(tweets_lowered)\n",
    "        #tweets_tokenized = word_tokenize(tweet)\n",
    "#         tokenizedTweet = [\"NORP\" if ent.label_ == \"PERSON \" else \"GEO\" if ent.label_ == \"ORG \" for ent in tokenizedTweet.ents]\n",
    "#         pprint([(X.text, X.label_) for X in doc.ents])\n",
    "        #tweets_tokenized = word_tokenize(tweet)\n",
    "#         spacy_process(tokenizedTweet)\n",
    "        tweets_tokenized = word_tokenize(str(tweets_lowered))\n",
    "        \n",
    "#         tweets_no_stopwords = [w for w in tweets_tokenized if w not in stop_words_en]\n",
    "#         tweets_no_special_char = [w for w in tweets_no_stopwords if w.isalnum()]\n",
    "        tweets_no_bads = [\"BAD_WORD\" if w in bad_words_array else w for w in tweets_tokenized]\n",
    "        tweets_corpus = [\"HATE_WORD\" if w in hate_words_array else w for w in tweets_no_bads]\n",
    "\n",
    "        #tweets_tokenized = word_tokenize(tweet)\n",
    "        #tweets_tokenized = word_tokenize(tweets_lowered)\n",
    "        #tweets_corpus = [w for w in tweets_tokenized if w not in stop_words_en]\n",
    "        # lemmatize words\n",
    "#         tokenizedTweet = nlpEN(tweet)\n",
    "        # Sentiment analyzer\n",
    "#         analyser = SentimentIntensityAnalyzer()\n",
    "#         score = analyser.polarity_scores(tweet)\n",
    "#         # Convert dictionary into string\n",
    "#         score = str(score) \n",
    "#         processedTweet = []\n",
    "#         for l in tweet:\n",
    "#             #processedTweet.append(f\"{l.lemma_}\")\n",
    "# #             processedTweet.append(f\"{l.lemma_}({l.pos_})\")\n",
    "#         processedTweet.append(score)\n",
    "        corpus.append(' '.join(tweets_corpus))\n",
    "    return corpus\n",
    "\n",
    "# def replace_person(token):\n",
    "#     if token.ent_iob != 0 and token.ent_type_ == 'PERSON':\n",
    "#         print(token.text)\n",
    "#         return 'PERSON_TAG'\n",
    "#     return token.text \n",
    "\n",
    "\n",
    "# def replace_geo(token):\n",
    "#     if token.ent_iob != 0 and token.ent_type_ == 'ORG':\n",
    "#         return 'GEO_TAG'\n",
    "#     return token.text \n",
    "\n",
    "# def spacy_process(doc):\n",
    "#     with doc.retokenize() as retokenizer:\n",
    "#         for ent in doc.ents:\n",
    "#             retokenizer.merge(ent)\n",
    "#     tokens = map(replace_person, doc)\n",
    "#     return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a dummy dataframe\n",
    "# training_data = pd.DataFrame()\n",
    "# # load the preprocessed text to it otherwise en_data['text'] stays the same\n",
    "# training_data['preprocessed_text'] = preprocess_(en_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(training_data['preprocessed_text'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_train_split = preprocess_(X_train)\n",
    "# proc_test_split = preprocess_(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the data after cleaning\n",
    "# # proc_train_split[0:5]\n",
    "# en_data[0:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.55\n"
     ]
    }
   ],
   "source": [
    "# run vsm on word ngrams\n",
    "# vectorizer_en = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,5), max_features=1000, stop_words=stop_words_en)\n",
    "# vectorized_training = vectorizer_en.fit_transform(proc_train_split).toarray()\n",
    "# vectorized_testing = vectorizer_en.transform(proc_test_split).toarray()\n",
    "# svr = SVC(kernel='linear', C=1000)\n",
    "# svr.fit(vectorized_training, y_train)\n",
    "# print(\"SVM:\" , accuracy_score(svr.predict(vectorized_testing),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.775\n"
     ]
    }
   ],
   "source": [
    "# # run vsm on char ngrams\n",
    "# vectorizer_en = TfidfVectorizer(analyzer='char', ngram_range=(2,5))#char2-5 \n",
    "# vectorized_training = vectorizer_en.fit_transform(proc_train_split).toarray()\n",
    "# vectorized_testing = vectorizer_en.transform(proc_test_split).toarray()\n",
    "# svr = SVC(kernel='linear', C=1000)\n",
    "# svr.fit(vectorized_training, y_train)\n",
    "# print(\"SVM:\" , accuracy_score(svr.predict(vectorized_testing),y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists from https://unicode.org/emoji/charts/emoji-list.html\n",
    "def face_neutral_skeptical(text):\n",
    "    return len([c for c in text if c in 'ð¤ð¤¨ððð¶ððð¬ð¤¥ð§'])\n",
    "\n",
    "def face_concerned(text):\n",
    "    return len([c for c in text if c in 'ðððâ¹ð®ð¯ð²ð³ð¥ºð¦ð§ð¨ð°ð¥ð¢ð­ð±ðð£ððð©ð«ð¥±ðð¿'])\n",
    "\n",
    "def face_negative(text):\n",
    "    return len([c for c in text if c in 'ð¤ð¡ð ð¤¬ðð¿ðâ ð¾'])\n",
    "\n",
    "def face_costume(text):\n",
    "    return len([c for c in text if c in 'ð©ð¤¡ð¹ðºð»ð½ð¾ð¤'])\n",
    "\n",
    "def body_parts(text):\n",
    "    return len([c for c in text if c in 'ðð¤ðâððð¤ð¤âð¤ð¤ð¤ð¤ðððððâððâðð¤ð¤ðððð¤²ð¤ððªððð'])\n",
    "\n",
    "def people(text):\n",
    "    return len([c for c in text if c in 'ð¶ð§ð¦ð§ð§ð±ð¨ð§ð©ð§ð´ðµððââï¸ðððððð§ðð¤¦ð¤¦ââï¸ð¤·ð¤·ââï¸'])\n",
    "\n",
    "def bad_words(text):\n",
    "    return len([c for c in text.split() if c == \"BAD_WORD\"])\n",
    "\n",
    "def hate_words(text):\n",
    "    return len([c for c in text.split() if c == \"HATE_WORD\"])\n",
    "\n",
    "def user_count(text):\n",
    "    return len([c for c in text.split() if c == \"user\"])\n",
    "\n",
    "def rt_counts(text):\n",
    "    return len([c for c in text.split() if c == \"rt\"])\n",
    "\n",
    "def url_counts(text):\n",
    "    return len([c for c in text.split() if c == \"url\"])\n",
    "\n",
    "def person_counts(text):\n",
    "    return len([c for c in text.split() if c == \"NORP\"])\n",
    "\n",
    "def geo_counts(text):\n",
    "    return len([c for c in text.split() if c == \"GEO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data['face_neutral_skeptical'] = data['preprocessed_text'].apply(face_neutral_skeptical)\n",
    "    data['face_concerned'] = data['preprocessed_text'].apply(face_concerned)\n",
    "    data['face_negative'] = data['preprocessed_text'].apply(face_negative)\n",
    "    data['face_costume'] = data['preprocessed_text'].apply(face_costume)\n",
    "    data['body_parts'] = data['preprocessed_text'].apply(body_parts)\n",
    "    data['people'] = data['preprocessed_text'].apply(people)\n",
    "    data['person_counts'] = data['preprocessed_text'].apply(person_counts)\n",
    "    data['geo_counts'] = data['preprocessed_text'].apply(geo_counts)\n",
    "    data['bad_words'] = data['preprocessed_text'].apply(bad_words)\n",
    "    data['hate_words'] = data['preprocessed_text'].apply(hate_words)\n",
    "    data['user_count'] = data['preprocessed_text'].apply(user_count)\n",
    "    data['rt_counts'] = data['preprocessed_text'].apply(rt_counts)\n",
    "    data['url_counts'] = data['preprocessed_text'].apply(url_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy dataframe\n",
    "training_data = pd.DataFrame()\n",
    "# load the preprocessed text to it otherwise en_data['text'] stays the same\n",
    "training_data['preprocessed_text'] = preprocess_(en_data['text'])\n",
    "# siena's counters function\n",
    "preprocess(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"BAD_WORD new york # url # # user # # user # i think i 'm in love trump is awesome # url # # user # you have the greatest tweets sweetheart # user # # user # it 's free pizza hun , just free food # user # love you joy can i help cnn , i can piss on ya # url # she is a wet potato sack # url # # user # long gone darlin i like them too # url # rt # user # : retweet if you think michael moore is a worthless loser ! # url # rt # user # : did cnn just re-unite the alt-right & amp ; the new right in a common cause ? ð¤ # url # oh , a willnot # url # rt # user # : hey # user # ! just saw a 12 year old with a video camera and a trump t-shirt , might want to pay this clown a visit # user # is so low , he can crawl under a snake and not alert it , better yet , he 's a goof # url # let me get that for ya honey # url # i 'll eat what the BAD_WORD i want # url # # user # you mean this BAD_WORD , yep you nailed it , he is an BAD_WORD # url # wow # user # at those jeans # url # what usa looks like today # url # rt # user # : tv ratings : fox news has 14 of the top 20 cable news shows in total viewers this quarter â # url # rt # user # : ivanka will be our next woman president ! # url # rt # user # : want to stop illegals from voting in 2018 wear an ice jacket when you vote # url # more drug addicts , ya , what a way to go # url # ai n't blacks the greatest , HATE_WORD people BAD_WORD # url # rt # user # : # user # you cocksucking whore # user # you cocksucking whore there clowns , everybody hates clowns # url # ivanka is such a beautiful women , how dare she # url # # user # 181 followers , you like me , you really really like me nice table cloth hillary , what diner did you steal that out of , egad # url # geez , be careful down there yanks , we canucks have 35 million people , every 8 hours we 're wiped out # url # rt # user # : president trump controls the treasury & amp ; should stop payment of congressional salaries until they actually work ! # hashtag # # user # hang in there beautiful , you 'll be fine as long as it ai n't rosie 's aroma i 'm fine # url # # user # # user # they should be encouraging antifa # user # # user # is a complete idiot liz , crack smoking BAD_WORD if i remember correctly # user # , BAD_WORD idiot # url # had the best father 's day with my daughter , i 'm so proud of her # user # love you # user # your an idiot # user # is going on yanks , pull your socks up . dems are running your country into the ground rt # user # : heads up to the gun grabbers ( aint gon na happen ) # url # rt # user # : the truth if i ever seen it ! # url # i thought cheerleaders were females # url # the left is mad max # url # rt # user # : this year trip to dc has been phenomenal ! i 'm so grateful ! thank you god for your many blessings ! # url # ai n't she a beaut , what a piece of BAD_WORD # url # as if anyone ver needed a reason # url # i love lori # url # rt # user # : retweet if you think that obama lackey loretta lynch should be indicted # url # # user # # user # you mean like , hey bud stop walkin , i got ta have a BAD_WORD , that sorta thing ? that 's nice rt # user # : this guy mouthed `` terrorist sympathiser '' behind corbyn ... # url # rt # user # : if you think # user # is a piece of BAD_WORD , is there anything he could do that would change your mind ? tissue anyone , egad , cry me a river . i guess its plan z1a2 , grow the f up lobs # url # god meant mars global warming , hurry up al the next flight is about to leave , ya putz # url # rt # user # : # user # what is n't wrong washed ð she just rambles on and has the wrong person she 's rambling about . ð¤ð¤ð¤ rt # user # : liars ! ! i will never vote democrat again # url # well , that 's reality # url # rt # user # : 71 % of people in poland want to ban muslim immigration # url # how the west can weather the loss of u.s. leadership /via # user # # url # the globe & amp ; email is a rag , oh it 's free too rt # user # : let 's salute the courageous female # hashtag # snipers as they force # hashtag # to retreat from # hashtag # # user # # user # is this her # url # rt # user # : grande to return to stage sunday for manchester benefit show # url # # url # rt # user # : theresa may wants longer prison sentences for terrorists . i 'm sure this will deter suicide bombers . rt # user # : better pace yourself ð # hashtag # # url # # user # # user # what a loser haha # url # how 's that jello pudding taste now ya BAD_WORD piece of BAD_WORD cosby , tell o j i said hello , sick BAD_WORD # url # i thought he threw the dog at the tv , my mistake # url # rt # user # : trump has tweets for every occasion . # url # # user # # user # # user # luck larry that is # user # # user # very well said # user # yes thank you bill and kat because she can ... . # url # # user # # user # r u fucked # user # # user # but he puts it back with natural gas that he spews surfboards ? check the speedo 's they 're wearing , i got ta get me one # url # but it has solar powered nose hair trimmers # url # # user # cereal , comes with free enema # url # rt # user # : yo # user # you gon be cancelled by tomorrow . black twitter bout to get you fired and find your address . wow , and it 's breaking , nbc must be hard at work snoring over the all investigation docs they 've uncovered # url # when did madcow get the scar # url # # user # # user # # user # hey kathy , if your looking for work , i could use my scrotum scrubbed , BAD_WORD skank # user # # user # lol , you kill me liz i love it , must watch # url # # user # , please dont put pics like this anymore , egad i 'm going to be sick , oh wow , i covfefe 'd myself # url # rt # user # : # user # # user # terrorist check list : plan in advance announce in advance commit act claim victim hood # user # # user # barron would kick the BAD_WORD out of her , you know libs , they think there tough and run when they 're stood up to rt # user # : kathy griffin just earned thousands of votes for president trump . kathy is your democrat party . well done . # hashtag # rt # user # : please raise your boys to be real men in a world where society urges you to raise a feminist son ! ! ðð»ðµ # url # # user # # user # # user # what about the goats you sleep with linda rt # user # : the moment she learned a formidable candidate is competing for her congressional seat . # url # rt # user # : after hillary 's speech today , remember this -- & gt ; cash flowed to clinton foundation amid russian uranium deal i love these two # url # ask the dems # url # children 's book author , who was his dad , caligula , what a goof , # user # pos # url # hurry up and ie # user # , nobody would care , loser # url # # user # is a disgusting pile of BAD_WORD , # url # # user # # user # well chuck , it 's like this , they 're stupid . do n't hold back # url # rt # user # : you have to admire his honesty . # url # ya george , stfu and quit being a crybaby # url # love the way the snowflakes fall and make an BAD_WORD of themselves # url # out of touck , ah , more out of there minds # url # haha , good one # url # rt # user # : # user # wore it better # url # # user # wore it better # url # # user # # user # # user # # user # # user # not to mention your gorgeous when you think of idiot , think of # user # , trust me , he 's an idiot who 's the idiot on the right , what an BAD_WORD # url # wow , hangings to good for em , oh the humanity # url # # user # # user # did he just say college campuses are n't a free speech area , i 'm confused i like that girl # url # yep , he did same to me # url # rt # user # : mainstream media completely silent on this story . # url # she 's fucked up # url # here we go with the racist BAD_WORD again # url # # user # oye that michael oboma is one ugly dude # user # i 'm boycotting any product that has a hollywood celebrity in its commercial , who 's with me rt # user # : doing the ( wrong ) math # hashtag # # url # # user # you have to be the dumbest blondes ive ever had to endure to listen to , egad go back to school i think stucky is stuck on stupid # url # rt # user # : `` if every repub candidate picked up a reporter n slammed them i would go bankrupt donating to them '' ð¤£ð­ð¤£ð i need a tissue # url # rt # user # : i support , i will leave too # url # who did you ask # user # a majority of 4 year olds # url # rt # user # : busted you BAD_WORD , guess what ! no special prosecutor . # url # rt # user # : when your mouth becomes full of ðð© it 's good to keep a refreshing drink nearbyâ¼ï¸ðð» # url # rt # user # : the real scoop.. f you you twerp , and i , m being nice ! # url # # user # # user # # user # # user # very well said rt # user # : is this what it is coming to before liberals get it ! # url # # user # # user # well said cassandra rt # user # : if you had any doubt , this should confirm that isis is responsible for the manchester attack . # url # rt # user # : our beautiful first lady touches the sacred and holy western wall . i love our # user # # url # george harrison - blow away # user # away hillary # url # why are n't they going after hillary , BAD_WORD i 'm sick of this BAD_WORD # url # sarcasm , i hope # url # # user # # user # # user # # user # # user # # user # # user # # user # same pant suit , same freak slick willy did it # url # not true , she 12 and a half # url # # user # i hope you do i love you â¤ï¸ # url # # user # # user # he ca n't act , BAD_WORD just lays there do n't ever come back to canada you piece of BAD_WORD , goof # user # # url # rt # user # : c'mon media stop hiding this story ... americans deserve to know the truth ! ! ð¡ # url # happy mothers day to every beautiful mom out there , wish i could hug every one of you i love you too # url # # user # # user # more like a hall monitor to bad oj did n't die 31 years ago # url # rt # user # : facts are tough on # hashtag # ... # url # # user # # user # # user # hahaha , # user # is the funniest idiots in the universe , what a clown rt # user # : boom ð i â¤ï¸ watching sarah huckabee sanders call out hypocritical democrats in the wh press briefing ! # url # # user # why does it take a liberal an hour and a half to watch # user # rt # user # : oh , these poor things.. # hashtag # soon they 'll run outta other peoples moneyð # url # you go girl , well done # url # rt # user # : if # hashtag # is n't killing , why is it considered a # hashtag # when a # hashtag # is murdered ? # hashtag # # user # # user # we all noticed # user # # user # # user # i ca n't believe the BAD_WORD lasted as long as he did why does n't he just come out and get it over with . we know colbert , your gay , we get it , now shut up already ... goof # url # eew , yuck , who let the pigpen door open , my god , what do you feed something like that # url # no offence but , pocahontas , women like you will make sure there never will be a woman prez . idiot # url # rt # user # : president trump is right about andrew jackson and the civil war , the morons on cable news are wrong . rt # user # : jesus loves you . but everyone else thinks you are an BAD_WORD ðð½ # hashtag # # user # # user # what 's google , must be another trump hater # user # # user # # user # really , tax returns , that 's all you got . get over it cry baby haha , cry babies # url # rt # user # : convicted terrorist who helped organize `` a day without a woman '' strike pleads guilty to immigration fraud # user # happy birthday lori # user # i 'm sure the big-wigs have there homes built now so that 's it , no more mansions rt # user # : 300 guitars for sale : a story of a manic buying spree # url # # url # rt # user # : andrew lumish , dedicates his one day off a week to restoring veteran 's tombstones in tampa , florida ! # url # # user # exactly # user # its getting old real fast shoot him again # url # i love easter # url # # user # # user # would you like a nice cold beer mr. prez rt # user # : william wilberforce , BAD_WORD . # url # maybe your world # url # # user # # user # i 'd love to warm her BAD_WORD pads , i love her rt # user # : hamilton judge who wore trump hat in court to face disciplinary hearing # url # rt # user # : wow # user # , that is a man 's handshake if i ever saw one ! ððð­ # hashtag # # hashtag # # url # rt # user # : imagine the hater 's face when they realize how i continue to utterly dominate them . ð # url # # user # # user # # user # # user # or spend some money on that ugly mugg of hers rt # user # : 39 % of young people in france aged 18-24 support marine le pen . populism is the new punk ! # url # rt # user # : justice ruth bader ginsburg , who insists that trump is unfit for his job . # url # rt # user # : dear muslims , before trying to fake civilian deaths , make sure the cameras are not rolling ! [ video ] â liberty i pretty much nailed it lizzy # url # un-fuckin believable , what the BAD_WORD . BAD_WORD i hate muslims # url # rt # user # : miracles do exist ! gloria being investigated for misconduct ððððððð # url # good luck with that # url # yikes , i 'm poor and i would n't do that thing , why would trump # url # # user # oh i 'm beginning to like you , ð # user # # user # that 's nice , do n't forget , your stupid too # user # # user # # user # you left out a couple words , like kill non-believers and BAD_WORD like that , losers # user # i 'll remember that , thanks stupid # user # # user # # user # do you like to beat your wife and kids ? that 's the best ever tweet # url # rt # user # : youtube ad boycott # url # ya , and boy o boy they 're gunna pay , BAD_WORD , give me a break # url # just call # user # stupid , he goes by that name # url # that guy is the scariest , i 'd break his BAD_WORD face if he ever went near my child # url # i knew she was # url #\",\n",
       " \"romanian graftbuster â s firing violated rights , european court says # url # russian ventilators sent to u.s. made by firm under u.s. sanctions : russia newspaper # url # hezbollah prevented isis from reaching europe : iranian official to germany # url # via # user # epidemiologist dr knut wittkowski : â lockdown has no benefit , only negative effects â # url # via # user # china refuses to let who investigate truth behind the origin of coronavirus outbreak # url # panorama reveals host of government failures to protect nhs workers # url # via # user # the challenge of counting covid-19 deaths # url # twitter suspends account of biotech company testing uv light to treat coronavirus # url # via # user # first volunteer in uk coronavirus vaccine trial has died # url # # url # coronavirus : china ups wuhan covid-19 death toll by 50 % 'disinformation ' : see trump virus briefing get cut off on live tv # url # via # user # dr. fauci reports that alcohol may help people survive coronavirus briefings # url # via # user # # url # 'we are only at the beginning of the coronavirus crisis , ' says economic historian breaking : new evidence shows cdc knew since at least 2005 that chloroquine is effective against coronaviruses # url # report : fbi warned of chinese researchers transporting disease samples in us before coronavirus # url # via # user # a test than can detect coronavirus in minutes has been approved in the us # url # via # user # coronavirus deaths rising faster in uk than italy # url # # url # germany mulls mass testing for coronavirus immunity : report # url # coronavirus : netherlands recalls 'defective ' masks bought from china â intense â spike in christian persecution after china â s secret deal with vatican : us gov â t report | news | lifesite # url # get daily updates on the epidemic and learn how it is affecting countries around the world . # url # this is how europe is helping companies and workers as the coronavirus crisis deepens # url # via # user # the eu travel ban explained # url # the propaganda virus : it is not the eu lacking solidarity , but the member states # url # first dose to be delivered monday in clinical trial for potential covid-19 vaccine # url # video footage of aftermath from us airstrikes on karbala airport # url # via # user # germany tries to halt u.s. interest in firm working on coronavirus ... # url # rt # user # : sweden : public health authority will no longer report number of covid-19 cases # url # eu candidate leaders : a dangerously comic cocktail of incompetence , treachery and globalism # url # pew poll analysis : a billion muslims want sharia law # url # via # user # eu leaders to agree on 10 commitments at summit : draft text # url # theresa may has been given a second chance to save brexit . she â d better not blow it # url # the eu â s hard-border bluff will soon collapse â and then it can get serious about brexit # url # rescued sea watch migrants to land in italy after deal reached : pm # url # europe still thinks britain will come out worse from brexit # url # migrants land in sicily as ship crew faces uncertain fate # url # uk backs 'targeted ' sanctions on venezuelan 'kleptocrats ' # url # germany : girl fleeing migrant attackers run over by bus # url # eu announces customs backstop alternative as rees-mogg expresses support # url # hundreds evicted as italy closes its second-largest migrant centre # url # via # user # franco-german treaty : macron & amp ; amp ; merkel join hands to raise a â european army â # url # italian intel : smugglers may arrange migrant deaths for public sympathy # url # via # user # new danish asylum curb could restrict refugee access to medicine and dental care # url # via # user # iraqi asylum-seeker charged with rapes & amp ; amp ; murder of 14-year old girl in germany # url # pregnant woman raped by 2 men - english - # url # # url # germany : muslim migrant charged with child rapes and murder # url # slovakia : arguably , the most anti-islam eu member state # url # bbc news - former scottish first minister alex salmond arrested # url # salvini refuses to reopen ports : stopping migrant boats saves lives # url # via # user # austrian vice chancellor slams pro-migration activists after murders # url # via # user # spain uses eu 'no deal ' brexit plans to push claims for the 'decolonisation ' of gibraltar # url # via # user # half of failed asylum seekers remain in uk illegally , says ex-immigration chief # url # via # user # theresa may to consider axeing human rights act after brexit , minister reveals # url # germany â s chancellor-in-waiting appeals to british public to think again and scrap brexit # url # portugal : violent african immigrants arrested after throwing stones at police # url # eu parliament backs punishing countries that reject progressive values # url # via # user # can new treaty fix a fraying europe ? # url # eu plot to block brexit : nigel farage reveals how brussels 'think brexit is dead ' # url # brexit live : may hit by trade disaster - shock border force warning sparks freight concern # url # israeli pm urges romania to move embassy to jerusalem # url # yellow vests protests continue despite president macron 's outreach : # url # via # user # rage of the tory grassroots and why brexit betrayal will consign party to oblivion # url # via # user # the eu has shown that it wants britain as a subject , not an ally # url # via # user # eu needs reason to extend brexit process , should britain ask - ... # url # germany could deport asylum-seekers who hide their identity # url # via # user # broken europe # url # via # user # french pm : we 're spending â¬50m preparing for no deal # url # emmanuel macron â s fear of frexit is bad news for britain # url # the cheer on question time that will terrify corbyn â s labour # url # eu states breaching rule of law could lose funds : parliament # url # end in sight for romania miners ' strike : ministry # url # how reported crime rates changed in sweden in 2018 # url # via # user # basic income , pension decree ready - english - # url # # url # british army reservists called up to help prepare for no-deal brexit # url # students march in paris to protest macron 's education reforms ( video ) : # url # via # user # syrian refugee gets jail term in belgium for terrorist attack plot - reports : # url # via # user # 300 % increase in migrants to greece ! turkey deliberately negligent on border with greece.l # url # eu civil war : brussels freezes cash in 'rogue state ' crackdown - hungary furious # url # 'bunch of oligarchs ! ' brexiteer reveals how eu officials are exempt from national taxation # url # one dutch trawler gets a quarter of england 's entire fish quota # url # via # user # dalston homophobic â acid attack â : gang of cowardly thugs jailed over â truly shocking â bank holiday assault # url # afghan migrant arrested after 87-year-old german woman brutally killed # url # via # user # german carmakers warn of â fatal â consequences of a no-deal brexit - 'jobs are on the line â # url # democracy is in danger as our political leaders seek to subvert the leave vote # url # via # user # ' c*** ' : twitter erupts over bbc newsreader mispronouncing uk foreign sec 's name : # url # via # user # macron 's ex-security aide in custody over probe into diplomatic passports : # url # via # user # merkels secret police announces plans to spy on political party afd - voice of europe # url # juncker regrets `` no deal '' on brexit in british parliament - voice of europe # url # migrants stole swedish woman 's puppies , beat her unconscious and tried to break her neck - voice of europe # url # new cases of child BAD_WORD revealed in finland - president says asylum seekers brought evil with them - voice of europe # url # salvini vows action on naples bomb - english - # url # # url # britain did n't accept a single christian refugee , accepted muslims only # url # german bank association says uk brexit vote is 'warning shot ' # url # eu prepares plan to end national vetoes on taxation via new europe # url # everything you need to know about the vote of no confidence in the government # url # bbc news - brexit : six things eu could do if theresa may 's deal gets voted down # url # brexit : meps pen letter urging britain to reverse decision to leave eu # url # greek pm set to scrape through confidence motion over macedonia deal # url # greece and russia exchange furious statements over macedonia # user # # url # brexit deal rejected by 432 votes to 202 # url # pensioners in bulgaria fourth-highest in eu at risk of poverty - eurostat # url # via # user # hey check this out # url # # url # +++ uk parliament brexit vote â live updates +++ greece , bulgaria , romania and serbia declare war on islam and its trojan ... # url # via # user # top bank warns clients to stop trading pound before brexit vote # url # may faces defeat in parliament over brexit plan # url # france : â yellow vest â protesters take to streets for 9th straight week # url # via # user # finnish president expresses his 'disgust ' at migrant grooming gangs # url # via # user # verona declares itself a â pro-life city â , and fights to prevent abortion # url # british lawmakers to vote on brexit deal # url # macron in desperate need for change , writes letter to the french public - voice of europe # url # `` der spiegel '' associates us ambassador in germany with neo-nazis - voice of europe # url # yellow vest protestor shot in the back of the head by french police # url # via # user # one-way ticket : croatia â s growing emigration crisis # url # macron is a traitor to HATE_WORD people everywhere # url # muslim preacher in london `` we are not british . we belong to isis '' - speech point - # url # # user # swedish right-wing led municipality ends pork-free school meals - voice of europe # url # protesters attempted to storm city hall as yellow vest protests entered 9th week in france - voice of europe # url # gloves on both sides are definitely off # url # dutch kept tribute for killed rapper â look what happens ... - voice of europe # url # asylum seeker attacks pregnant woman # url # german town in shock after refugees hospitalised 9 people during â BAD_WORD of violence â # url # # url # mayor of gdansk dies of stab wounds after attack marine le pen unveils far-right candidates for european election # url # the worst brexit option , except for all the others # url # via # user # pope francis to visit romania : vatican # url # stranded migrants disembark in malta after eu deal reached # url # dutch pm chides 'white wine-sipping elite ' for trump-bashing # url # italian league senator convicted for racist remarks # url # ahead of vote , may warns it would be catastrophic to halt brexit # url # polish mayor dies after being stabbed on stage at charity concert # url # afd party votes to campaign for german exit from eu # url # finland : muslim refugee `` i 'd rather die than get a job and adopt european culture '' - speech point - # url # # user # german leader says germans are fed up with mass immigration `` merkel has to go '' - speech point - # url # # user # sweden : 116 hand grenade attacks in the last 7 years # url # traffickers â guarantee uk residence â , as britain â never sends you back # url # via # user # eu offers brexit reassurance but no changes to deal # url # greek pm tsipras to face confidence vote wednesday # url # flemish government chief to lead n-va 's eu list # url # germany 's far-right afd to campaign on possible eu exit # url # europe : more nifty censorship from the eu : # url # minaret daggers at europe â s throat # url # via # user # bbc news - pawel adamowicz , stabbed gdansk mayor , dies of injuries # url # un migration pact , what exactly is it ? # url # in soviet sweden , a social media snitch is a national hero # url # eu may need rules to stop doctors emigrating - german minister # url # yellow vests : act ix # url # french police using semi-automatic weapons for yellow vest protestors # url # via # user # german newspaper says supporters of party opposed to mass muslim migration should be denied right to vote # url # after muslim violence , europeans are getting guns # url # court hears mohammed abdul told bouncers â i â ll kill you â , drove into club # url # via # user # uproar as islamists reportedly spotted at muslim meeting in cologne , germany : # url # via # user # democracy ? france announces new measures to curb yellow vest protests # url # afd leader blames media smear campaign for â assassination attempt â on state chairman # url # bulgarian truck driver beaten by migrants in france - # url # - sofia news agency # url # majority of germans feel strangers in their own country since the influx of muslim migrants - study # url # kidnappers de cererea de victoria norwegian billionaire crypto ransom # url # # url # romanian anti-corruption chief steps down # url # athens blanketed with unaccustomed snow # url # ten men go on trial accused of preying on two vulnerable girls in care # url # italy set to launch hydrocarbon exploration # url # via # user # bbc news - brexit : 20 tory rebels inflict no-deal defeat on government # url # # url # what made austria 's maria theresa a one-of-a-kind ruler germany tells british mps to 'be responsible ' and back theresa may 's brexit deal # url # via # user # bbc news - bradford grooming trial : ten men accused of abusing girls in care # url # eu moves closer to tightening rules on london-based investment firms # url # bbc news - us downgrades eu diplomatic status in washington # url # british officials 'putting out feelers ' with eu for article 50 extension # url # via # user # warnings from versailles # url # via # user # delay brexit ? ireland would not stand in the way # url # romanian engineer who saved churches during communist era dies # url # some migrants stranded off malta refusing food : sea-watch # url # belgium bans halal and kosher slaughter # url # # url # trump administration downgrades eu mission to us bbc news - yellow vests : france to crack down on unsanctioned protests # url # greek priest beaten by syrian asylum seekers in front of his own church # url # via # user # â president against his people â : salvini openly backs yellow vest protesters , lashing out at macron â rt world news # url # france and germany will find the eu â frightening â without the uk claims baroness meyer # url # prepare for a wto brexit and liberation ! # url # romania takes over eu presidency during turbulent times # url # eu politicians to join anti-hungarian government protests 'to serve interests of george soros ' # url # italy 's salvini keeps campaign promise and reduces number of migrant crossings by 80 per cent # url # watch : shocking footage shows how police officers get attacked by â arabs â in berlin # url # migrant involved in 'torturing ' dog to death in sweden # url # professors warn : german welcome culture is over , people focus on resistance and national identity # url # 'dark day for democracy ' : masked men nearly beat right-wing leader to death in 'politically motivated attack ' # url # brexit live : desperate germany makes brexit plea and admits 'eu 's foundations are shaken ! ' # url # romanian mum allowed to keep uk benefits despite never living here # url # france : 'yellow vest ' protesters storm ministry in paris # user # # url # poland to increase number of lobbyists in brussels # user # # url # chairman of germany 's far-right afd party is â beaten nearly to death â # url # via # user # french ministers aghast at support for boxer who fought police # url # watch : video of pro boxer joining yellow vests against french police goes viral # url # east london tube station on lockdown after stabbing reports # url # via # user # watch : anelga merkel says `` we must accept that refugees are more criminal '' - speech point - # url # # user # `` the party is over for the left-wing mayors who benefited from migrant business '' - italy 's salvini # url # two syrian refugees beat up an orthodox priest in athens , greece # url # hungarian opposition calls for nationwide protests # url # christians in poland celebrate three kings day # url # swedish police union calls fireworks ' a threat to democracy ' after numerous attacks in suburbs # url #\",\n",
       " \"`` hey jamal ( snickering uncontrollable ) you want some ( pfff ) lemonade ! '' what an idiot ! # url # rt # user # : cotton coming out with a banger # url # this is meant to be sarcasm but it 's a good point considering how underwhelming the pandemic has been # url # nick really just compared homosexuality to people shooting themselves in the headðð­ protect america first ! let 's go ! ! ! ! ! ! ! ! # url # all these fears about safety but a total refusal to address the problem is nonwhite immigrants # url # people will notice . it just takes time . # user # i thought people were doing it to prevent getting reported , not for fun why does anyone give woman writers attention like somehow this is supposed to be about a sunset ( ? ? ? ) # url # this tweet is so jewish it hurts # url # what a nightmare # url # i hate english class so much . anti straight man anti HATE_WORD BAD_WORD class . ca n't even get good grades in it me tuning in to # url # at 8:30 knowing the show will start an hour late again # url # # user # they used to teach race realism in their schools ! this just proves that the js are anti-american # url # # user # idk what happened but all of a sudden he 's much happier reading superchats what , get a life reply guy # url # rt # user # : nick fuentes - the great reset # user # # url # # user # it 's survival for me too . if other races multiply faster and mine misses out on reproduction due to homosexuality , we die out . # user # well , there 's ebonics ... worse to people who deserve it , like you # url # then why are you quoting christ if you do n't appreciate christianity , degenerate # url # # user # you had the opportunity to change it after you got purged # user # coomers posting their ls online # user # do n't ask their grandfathers about the 1930s pretty nice right now that nick has such a good attitude for superchats when there are so many so late got ta love the chat spamming judeo-christian as nick talks about s. king # user # he said groypers stormed the capitol and called us `` sick antisemites '' . the account that had the clip got purged . # user # could end mudsharking # user # communists on reddit started it . they 're trying to dilute his message with nonsense . # user # i got this background # url # # user # it 's okay to be w- # user # it 's a conference `` against cancel culture '' that canceled that crowd . there 's nothing wrong with giving the event a hard time . i did n't realize the purge would come so soon ! # url # you rock # user # ! # hashtag # goodness , we 're extremists ! awesome ! i 'll never forget the time someone wore a shirt to school that said , `` i 'm jewish . wan na check ? '' ð he was not actually jewish # user # yeah , over an hour late and in his car , no superchats you guys want a whitepill ? # url # # user # this edit is still super cool rt # user # : new comic by hedgewik ( his account is locked atm ) # url # # user # i 'm gon na become a waiter to pick up a based karen that wayðð»ðð»ð³ god i hope so # url # a man only wants one thing # url # # user # you only have half of la-1 colored in i just found this awesome picture of # user # # url # # user # my favorite example is to look up `` yt people '' in the twitter search # user # americanism not globalism â¦and now they 're using the pandemic as an excuse for even more demographic displacement # url # reddit is peak # user # # url # is lugenprasse an acceptable term in our circles or is it too unoptical # user # assuming biden does n't make them legalð¬ # hashtag # whomever sent in this superchat sure is getting his money 's worth ! redditors mad at iq . what 's new , # user # ? # url # why would they even write this ? does anyone care at all ? # url # # user # your wish was granted # user # `` i love the usa '' hits different these days better electric shock than eternal flamesð¤·ð»\\u200dâï¸ # url # # user # spam in his comments what 's happening to stonetoss today is exactly why nick fuentes mass blocks . # url # we do not want america ... ... to keep cutting out the stream ! # hashtag # # user # i 'm so glad you 're back to est post nick is personally mentioned in the highest court and that is n't even the main story tonight # hashtag # rt # user # : hello , based department ? # url # best superchat attitude in a long time tonight # hashtag # i do n't want to hear this ! i do n't want to hear this ! # hashtag # # user # # user # did you absorb him , wth # hashtag # nick is 100 % right about making good use of this time . i 've been reading much more of the bible . o7 button going crazy rn # url # # user # you have to meet # user # # user # the same way i got you to # url # i let him know ! # url # # url # rt # user # : have n't watched the super bowl in years ... but this year , it 's personal ! # url # # user # that 's got ta hit close to home for you i 've been hearing way too much `` it 's a bad translation ! '' lately . like , no . # url # `` yeah , it 's a mess '' ( further messes it up ) # hashtag # viewers are n't liking this because it 's unoptical who would 've thought africa needed more black ? # url # no okayyyyy at all tonight ! # hashtag # # url # fundraising for legal purposes ? # hashtag # this is just as effective as the dlive chat # hashtag # # url # if it 's not based it 's cringe `` is me banging a dog the same as a black person ? '' no , but both would be degenerate # hashtag # nick said , `` okayyyyy '' like , 20 minutes ago # hashtag # exactly how long did 8:00 start times last in 2021 ? # hashtag # the more i see homosexuality the more i want to force every single one of them into violent and invasive conversion therapy # user # you better not start copying across the sea # user # i think `` this means i 'm one step closer to being able to say the you-know-what-word '' going through yearbooks and in hindsight i 'm realizing one of my first grade teachers was a gigachad # url # i have my own platform now so i can say it # hashtag # i think the easiest way for mexicans to not get shot in walmart ... is for them to not be here ! # hashtag # oooooooo you know it 's like the new af lobby music is better # hashtag # missing out again ! # url # why are no girls at my school like this ? why must they all be degenerates ! ? # url # # user # i 'm just hoping my french teacher was right about the extreme right # user # do n't have a smartphone i have n't watched america first all week because i keep getting error ! remember in this championship game that the chiefs ' # hashtag # retweeted negative garbage about jaden mcneil last summer . mainstream conservatives were right about the job losses and such with the new administration . now let 's hope they 're right about the racism # user # early life check # user # but which one is realistic # user # `` that is disgusting ! '' # user # this dude 's about to go spout conspiracy theories on # url # # user # stuck at af cube and error message # url # 1 peter 2:17 an early sign for me to move farther right was when i could only name cons and not pros of diversity in social studies it 's hilarious thinking about how i 'm doing this for school as nick shouts an illustration for my notes today # url # i have to watch the inauguration for social studies class and you know i 'm watching the america first stream he said it again ! # hashtag # did n't he have jaden and scott on a few months ago ? # hashtag # . # user # is americafirst live equipped to stream the inauguration ? # user # it came on for me # user # but think of the birthrates ! when i get called down to dinner at 5:00 # url # # user # thinly veiled hatred . they hate mayonnaise . was going to attach a picture butâ¦why would i do that ate ribs for dinner and my lips are wrecked , good thing i have 4 chapstick flavors going through my old yearbooks , it 's insane how much of a degenerate i almost became before finding here . pretty glad there 's no live chat on a f dot live . my small computer could n't take it ! classmate answers making me want to fedpost what i turned in as a picture that represents the events at the capitol # url # # user # that 's what i put should i be optical or honest ? # url # # user # i 've told people to read fbi crime stats though # user # i 'm seeing the lobby but the music jumps from time to time the sooner our wayward brothers realize this , the better.âð» # url # this but unironically # url # # user # he does n't even need a firearm to do that i see a lot of people talking big about how much they want to fight k. rittenhouse . i 'm sure he would beat those fatsoes senseless i did some trolling on a non-political account of mine # url # i miss the pence that wanted to [ redacted ] the gays it better be # url # follower count dropping , looks like i wo n't even be hitting 200 here . at least i got to triple digits it 's gone now ... # url # rewatching last night 's america first . how this man talks this consistently well for so long is mind-blowing . # user # no one else but unironically this is unironically making me really sad # user # no family yet but i would be surprised if they do n't start one # user # the only mormon guy i know married a mexican womanð¬ official reports are going to say that those who died at the capitol yesterday died of covid # user # # user # you 're wrong , blm rioters ' charges were all dropped and they got billions from corporations rt # user # : â acabâexcept when they kill people i disagree with â # user # the american people will come first once again ! bro , we 're gon na get in so much trouble # url # no charges dropped for our guys that they catch , calling it now this aged well . outside of our circles they 're panicking # url # # user # # user # is the future new background for virtual school # url # my device crashed like hell while tweeting that too , might be telling me something if this is true then where 's my HATE_WORD gf ? ? ? # url # # user # now looking back i 'm realizing they 're all BAD_WORD addicts based on what they expect people to get into # user # west florida does n't include the isle of orleans , so you do n't get anything south of lake pontchartrain # user # america first compound # user # is this what you were looking for ? # url # # user # i got you # user # you know an english class BAD_WORD when it 's english class # user # this is the right take it 's been pretty dry here lately . just 3 days until the real excitement though ! # user # he supports trump , of course it 's for the platinum plan and such but it 's better than most alternatives # user # no clue . it was leftists that made that graphic though , so i would n't expect honesty . # user # we 'll punish them from now on ! this but unironically # url # # user # last year in social studies i saw a prageru video that unironically argued that the 3/5 compromise was anti-racist # user # # user # one with objective standards # user # why is hawaii f-tier , and why is n't louisiana consistent with other blue seal flags i made a new video for ( ( ( replies ) ) ) : this nose knows # url # `` they would complain if obama did that . why is no one complaining about trump golfing ? '' asks the journalist , complaining about trump golfing it took me 10 minutes to find this because of all the non-whites ' terrible new edits clogging the search results # url # # user # the next bobby jindal ! stonetoss nick fuentes ! # url # i love it when alex jones says `` nick fwintez '' # user # no dude , no one 's gon na fight on christmas , it 's an unspoken rule how have i never heard about this until now ? # url # # user # yes . one of my best friends at school is one i 'm gon na use this image with text about how i hate some group of people and get tens of thousands of likes # url # # user # the water is n't as shallow so it 's not as easy # user # first stream i watched i actually showed up about 15 minutes early ... then waited an hour of free men talking very long and passionate rant here in the middle of superchats . n. fuentes ending 2020 america first with a bang ! definitely having one of the latter moments with these hateful african-appropriated `` memes '' # url # wow , nick 's streaming now ! i went on to see a replay and he 's live , 11 in the morning ! man , leftists really think madison cawthorn is a nazi . just proof that you can be optical and still be threatening hope he 's in rehab for being an unfunny HATE_WORD merchant # url # that cringebag nurse privated account . at least we wo n't have to see that face anymore ! obviously nothing happened but they 're still acting like it 's real # url # do n't check this if you 're prone to fedposting . talks of non-whites taking over europe and enslaving us # url # # user # well , there are other america firsters who would do a good job . but no one else . # user # bleeding over to satellite tv , what i 'm watching now 3 commercials with broken branch couples in quick succession . not cool , advertisers ! # user # ngl you 're glowing with that comment # user # that 's just how china 's shaped , tibet included , but at an angle bruh # user # ð you really knew when his story took place # user # he had the look # url # # user # canada 's weak for giving all that to the indians it 's not even longer than normal , and i saw something about 4 hours . i 'm watching last night 's america first since you guys thought it was so powerful . i do n't get it . it seems pretty average to me . # user # do n't have girls on sc or enough people who would see it in time anyone know someone who goes or went by nh groyper ? just found an old screenshot . i inverted my blackpill graphic . whitepills now ! # url #\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of preprocessed data\n",
    "list(training_data['preprocessed_text'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>face_neutral_skeptical</th>\n",
       "      <th>face_concerned</th>\n",
       "      <th>face_negative</th>\n",
       "      <th>face_costume</th>\n",
       "      <th>body_parts</th>\n",
       "      <th>people</th>\n",
       "      <th>person_counts</th>\n",
       "      <th>geo_counts</th>\n",
       "      <th>bad_words</th>\n",
       "      <th>hate_words</th>\n",
       "      <th>user_count</th>\n",
       "      <th>rt_counts</th>\n",
       "      <th>url_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAD_WORD new york # url # # user # # user # i ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>197</td>\n",
       "      <td>59</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>romanian graftbuster â s firing violated right...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>`` hey jamal ( snickering uncontrollable ) you...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that BAD_WORD still fried to me the homie let ...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>poc love talking about police brutality but no...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>78</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>me and kayleigh just waking up lol we finna be...</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>168</td>\n",
       "      <td>155</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>i had BAD_WORD going on 2020 summer was fun no...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>rt # user # : contact your legislators and let...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>93</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rt # user # : i 'm sorry but , what ? # url # ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>339</td>\n",
       "      <td>94</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td># user # sir , little smile pls # user # princ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>76</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     preprocessed_text  \\\n",
       "0    BAD_WORD new york # url # # user # # user # i ...   \n",
       "1    romanian graftbuster â s firing violated right...   \n",
       "2    `` hey jamal ( snickering uncontrollable ) you...   \n",
       "3    that BAD_WORD still fried to me the homie let ...   \n",
       "4    poc love talking about police brutality but no...   \n",
       "..                                                 ...   \n",
       "195  me and kayleigh just waking up lol we finna be...   \n",
       "196  i had BAD_WORD going on 2020 summer was fun no...   \n",
       "197  rt # user # : contact your legislators and let...   \n",
       "198  rt # user # : i 'm sorry but , what ? # url # ...   \n",
       "199  # user # sir , little smile pls # user # princ...   \n",
       "\n",
       "     face_neutral_skeptical  face_concerned  face_negative  face_costume  \\\n",
       "0                         0               2              1             1   \n",
       "1                         0               0              0             0   \n",
       "2                         2               2              1             0   \n",
       "3                         1               6              2             0   \n",
       "4                         0               0              7             0   \n",
       "..                      ...             ...            ...           ...   \n",
       "195                       3              38              0             0   \n",
       "196                       1               2              0             0   \n",
       "197                       0               1              0             0   \n",
       "198                       0               2              0             0   \n",
       "199                       1               3              0             1   \n",
       "\n",
       "     body_parts  people  person_counts  geo_counts  bad_words  hate_words  \\\n",
       "0            12       3              0           0         33           1   \n",
       "1             0       0              0           0          2           1   \n",
       "2             3       4              0           0          3           3   \n",
       "3             9      22              0           0        117          31   \n",
       "4             8       1              0           0         50          78   \n",
       "..          ...     ...            ...         ...        ...         ...   \n",
       "195           4      32              0           0         57           8   \n",
       "196          15      36              0           0         34           6   \n",
       "197           0       2              0           0          5           0   \n",
       "198           0       1              0           0         11           9   \n",
       "199          16       4              0           0          2           0   \n",
       "\n",
       "     user_count  rt_counts  url_counts  \n",
       "0           197         59         131  \n",
       "1            58          2         205  \n",
       "2            94          6          69  \n",
       "3            59         28          20  \n",
       "4            15          6         121  \n",
       "..          ...        ...         ...  \n",
       "195         168        155          14  \n",
       "196          48         45           7  \n",
       "197         140         93         168  \n",
       "198         339         94          93  \n",
       "199         210         76         124  \n",
       "\n",
       "[200 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how the data now looks\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>Fuck New York #URL# #USER# #USER# I think I'm ...</td>\n",
       "      <td>043e2766cc6d22ae4e447ca5f2885a2a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Romanian graftbusterâs firing violated rights,...</td>\n",
       "      <td>06893abba0bb8f94fed7562350233ed7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Hey Jamal (snickering uncontrollable) You wan...</td>\n",
       "      <td>0a3ce42bea89e2a92a28f685735e605e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>That shit still fried to me the homie let one ...</td>\n",
       "      <td>0a6700c6023c6249bcc5820e2f5ee0de</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>POC love talking about police brutality but no...</td>\n",
       "      <td>0d02a3f644c9313315ecc6655ccfa3b9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>me and kayleigh just waking up lol we finna be...</td>\n",
       "      <td>f91fa8ecdd2440eb163516769573f24a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>I had shit going on 2020 summer was fun no lie...</td>\n",
       "      <td>fdb47a3f65091b9a5b989e1722c9fac4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>RT #USER#: Contact your legislators and let th...</td>\n",
       "      <td>fdb9f16899e3097e6db1f6a13d3572f8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>RT #USER#: I'm sorry but, what? #URL# #URL# RT...</td>\n",
       "      <td>fdef657f264ca50bc7b21574b24f82ab</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>#USER# Sir , little smile pls #USER# Prince , ...</td>\n",
       "      <td>feab35da86bf5f84085fb670cb2866e4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang class                                               text  \\\n",
       "0     en     1  Fuck New York #URL# #USER# #USER# I think I'm ...   \n",
       "1     en     0  Romanian graftbusterâs firing violated rights,...   \n",
       "2     en     1  \"Hey Jamal (snickering uncontrollable) You wan...   \n",
       "3     en     1  That shit still fried to me the homie let one ...   \n",
       "4     en     1  POC love talking about police brutality but no...   \n",
       "..   ...   ...                                                ...   \n",
       "195   en     1  me and kayleigh just waking up lol we finna be...   \n",
       "196   en     0  I had shit going on 2020 summer was fun no lie...   \n",
       "197   en     1  RT #USER#: Contact your legislators and let th...   \n",
       "198   en     0  RT #USER#: I'm sorry but, what? #URL# #URL# RT...   \n",
       "199   en     0  #USER# Sir , little smile pls #USER# Prince , ...   \n",
       "\n",
       "                            author_id  author  \n",
       "0    043e2766cc6d22ae4e447ca5f2885a2a       1  \n",
       "1    06893abba0bb8f94fed7562350233ed7       0  \n",
       "2    0a3ce42bea89e2a92a28f685735e605e       1  \n",
       "3    0a6700c6023c6249bcc5820e2f5ee0de       1  \n",
       "4    0d02a3f644c9313315ecc6655ccfa3b9       1  \n",
       "..                                ...     ...  \n",
       "195  f91fa8ecdd2440eb163516769573f24a       1  \n",
       "196  fdb47a3f65091b9a5b989e1722c9fac4       0  \n",
       "197  fdb9f16899e3097e6db1f6a13d3572f8       1  \n",
       "198  fdef657f264ca50bc7b21574b24f82ab       0  \n",
       "199  feab35da86bf5f84085fb670cb2866e4       0  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how the data used to look\n",
    "en_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face_neutral_skeptical</th>\n",
       "      <th>face_concerned</th>\n",
       "      <th>face_negative</th>\n",
       "      <th>face_costume</th>\n",
       "      <th>body_parts</th>\n",
       "      <th>people</th>\n",
       "      <th>person_counts</th>\n",
       "      <th>geo_counts</th>\n",
       "      <th>bad_words</th>\n",
       "      <th>hate_words</th>\n",
       "      <th>user_count</th>\n",
       "      <th>rt_counts</th>\n",
       "      <th>url_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>197</td>\n",
       "      <td>59</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>78</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>168</td>\n",
       "      <td>155</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>93</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>339</td>\n",
       "      <td>94</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>76</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     face_neutral_skeptical  face_concerned  face_negative  face_costume  \\\n",
       "0                         0               2              1             1   \n",
       "1                         0               0              0             0   \n",
       "2                         2               2              1             0   \n",
       "3                         1               6              2             0   \n",
       "4                         0               0              7             0   \n",
       "..                      ...             ...            ...           ...   \n",
       "195                       3              38              0             0   \n",
       "196                       1               2              0             0   \n",
       "197                       0               1              0             0   \n",
       "198                       0               2              0             0   \n",
       "199                       1               3              0             1   \n",
       "\n",
       "     body_parts  people  person_counts  geo_counts  bad_words  hate_words  \\\n",
       "0            12       3              0           0         33           1   \n",
       "1             0       0              0           0          2           1   \n",
       "2             3       4              0           0          3           3   \n",
       "3             9      22              0           0        117          31   \n",
       "4             8       1              0           0         50          78   \n",
       "..          ...     ...            ...         ...        ...         ...   \n",
       "195           4      32              0           0         57           8   \n",
       "196          15      36              0           0         34           6   \n",
       "197           0       2              0           0          5           0   \n",
       "198           0       1              0           0         11           9   \n",
       "199          16       4              0           0          2           0   \n",
       "\n",
       "     user_count  rt_counts  url_counts  \n",
       "0           197         59         131  \n",
       "1            58          2         205  \n",
       "2            94          6          69  \n",
       "3            59         28          20  \n",
       "4            15          6         121  \n",
       "..          ...        ...         ...  \n",
       "195         168        155          14  \n",
       "196          48         45           7  \n",
       "197         140         93         168  \n",
       "198         339         94          93  \n",
       "199         210         76         124  \n",
       "\n",
       "[200 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the features is everying in training data minus the first column\n",
    "features = training_data.drop(['preprocessed_text'], axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, en_data['author'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vectorizer_en = TfidfVectorizer(analyzer='char', ngram_range=(2,5))#char2-5 \n",
    "# vect_x_train = vectorizer_en.fit_transform(X_train).toarray()\n",
    "# vect_x_test = vectorizer_en.transform(X_test).toarray()\n",
    "\n",
    "svr = SVC(kernel='linear', C=1000)\n",
    "svr.fit(X_train, y_train)\n",
    "print(\"SVM:\" , accuracy_score(svr.predict(X_test),y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_ML_model = SVC(C=1000, kernel='linear')\n",
    "en_ML_model.fit(X_train,y_train)\n",
    "print(\"SVM:\" , accuracy_score(en_ML_model.predict(X_test),y_test))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "en_ML_model = LogisticRegression()\n",
    "en_ML_model.fit(X_train,y_train)\n",
    "print(\"LR:\" , accuracy_score(en_ML_model.predict(X_test),y_test))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "en_ML_model = ensemble.RandomForestClassifier()\n",
    "en_ML_model.fit(X_train,y_train)\n",
    "print(\"Random forest:\" , accuracy_score(en_ML_model.predict(X_test),y_test))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "en_ML_model = DecisionTreeClassifier()\n",
    "en_ML_model.fit(X_train,y_train)\n",
    "print(\"Decision Tree:\" , accuracy_score(en_ML_model.predict(X_test),y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below this line is old code - do not run ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypperparameter tuning section - too expensive to run\n",
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto'),\n",
    "        'params' : {\n",
    "            'C': [1,10,20,50,100,200,500,1000],\n",
    "            'kernel': ['rbf','linear']\n",
    "        }  \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': ensemble.RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators': [1,5,10, 50, 100, 200, 400, 500, 700]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1,5,10, 50, 100,500]\n",
    "        }\n",
    "    },\n",
    "    'naive_bayes_gaussian': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'naive_bayes_multinomial': {\n",
    "        'model': MultinomialNB(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'criterion': ['gini','entropy'],\n",
    "        }\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-e86a48752d75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         evaluation_en.append({\n\u001b[0;32m      7\u001b[0m             \u001b[1;34m'model'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hypperparameter tuning section - too expensive to run\n",
    "evaluation_en = []\n",
    "for model_name, mp in model_params.items():\n",
    "        clf = GridSearchCV(mp['model'], mp['params'], cv= 10, return_train_score=False )\n",
    "        clf.fit(X_train,y_train)\n",
    "        evaluation_en.append({\n",
    "            'model': model_name,\n",
    "            'best_score': clf.best_score_,\n",
    "            'best_params': clf.best_params_\n",
    "        })\n",
    "df = pd.DataFrame(evaluation_en, columns=['model', 'best_score', 'best_params'])\n",
    "print('Best English parameters')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Training Dataset\n",
    "en_training_div_data, en_testing_div_data = train_test_split(create_data_frame(\n",
    "                                            r\"C:/Users/VivAndMourhaf/PycharmProjects/HelloWorld/data/pan21/en\"), train_size=0.70)\n",
    "print(\"English training data split size\", len(en_training_div_data))\n",
    "print(\"English testing data split size\", len(en_testing_div_data))\n",
    "en_training_div_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish Training Dataset\n",
    "es_training_div_data, es_testing_div_data = train_test_split(create_data_frame(\n",
    "                                            r\"C:\\Users\\loren\\Dropbox\\Estudios\\Universidad\\Uni TÃ¼bingen\\ISCL\\Projects\\PAN\\Data\\es\"), train_size=0.70)\n",
    "print(\"Spanish training data split size\", len(es_training_div_data))\n",
    "print(\"Spanish testing data split size\", len(es_testing_div_data))\n",
    "es_training_div_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution\n",
    "# Data distribution is exactly even\n",
    "print('English distribution')\n",
    "print(sns.countplot(en_training_div_data.author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spanish distribution')\n",
    "print(sns.countplot(es_training_div_data.author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet length distribution\n",
    "# Observation: most tweet bodies are between 8000 and 20000 chars\n",
    "# Spanish (orange) tweets tend to be slightly longer\n",
    "en_training_div_data['tweet_length'] = en_training_div_data['text'].str.len()\n",
    "sns.distplot(en_training_div_data['tweet_length']).set_title('Tweet length distribution')\n",
    "es_training_div_data['tweet_length'] = es_training_div_data['text'].str.len()\n",
    "sns.distplot(es_training_div_data['tweet_length']).set_title('Tweet length distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(words):\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, collocations=False).generate(words)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('English non hate spreaders common words:')\n",
    "subset=en_training_div_data[en_training_div_data.author==0]\n",
    "text=subset.text.values\n",
    "words =\" \".join(text)\n",
    "create_wordcloud(words)\n",
    "\n",
    "print('English hate spreaders common words:')\n",
    "subset=en_training_div_data[en_training_div_data.author==1]\n",
    "text=subset.text.values\n",
    "words =\" \".join(text)\n",
    "create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spanish non hate spreaders common words:')\n",
    "subset=es_training_div_data[es_training_div_data.author==0]\n",
    "text=subset.text.values\n",
    "words =\" \".join(text)\n",
    "create_wordcloud(words)\n",
    "\n",
    "print('Spanish hate spreaders common words:')\n",
    "subset=es_training_div_data[es_training_div_data.author==1]\n",
    "text=subset.text.values\n",
    "words =\" \".join(text)\n",
    "create_wordcloud(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus functions for English and Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim only accepts tokenized lists\n",
    "# so this preprocesses and coverts tweet-body strings into tokens\n",
    "def build_tokenized_corpus_en(data):\n",
    "    corpus = []\n",
    "    for tweets in data:\n",
    "        tweets_lowered = tweets.lower()\n",
    "\n",
    "        # Further tweet sanitation\n",
    "        tweet = re.sub(r'\\s+[a-z]\\s+', ' ', tweets_lowered) # remove single characters like i and a\n",
    "        tweet = re.sub(r'^[a-z]\\s+', ' ', tweet)  # remove single characters at the beginning like i and a\n",
    "        tweet = re.sub(r'\\s+', ' ', tweet)  # remove extra spaces\n",
    "\n",
    "        #tweets_tokenized = word_tokenize(tweet)\n",
    "        #tweets_tokenized = word_tokenize(tweets_lowered)\n",
    "        #tweets_corpus = [w for w in tweets_tokenized if w not in stop_words_en]\n",
    "        # lemmatize words\n",
    "        tokenizedTweet = nlpEN(tweet)\n",
    "        # Sentiment analyzer\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        score = analyser.polarity_scores(tweet)\n",
    "        # Convert dictionary into string\n",
    "        score = str(score) \n",
    "        processedTweet = []\n",
    "        for l in tokenizedTweet:\n",
    "            #processedTweet.append(f\"{l.lemma_}\")\n",
    "            processedTweet.append(f\"{l.lemma_}({l.pos_})\")\n",
    "        processedTweet.append(score)\n",
    "        corpus.append(processedTweet)\n",
    "    #return corpus\n",
    "        #corpus.append(tweets_corpus)\n",
    "\n",
    "    return corpus\n",
    "    #tweets_lowered = [tweets.lower() for tweets in data]\n",
    "    #tweets_tokenized = [word_tokenize(tweets) for tweets in tweets_lowered]\n",
    "    #tweets_corpus = [w for w in tweets_tokenized if w not in stop_words_en]\n",
    "\n",
    "\n",
    "    #return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Spanish corpus\n",
    "def build_tokenized_corpus_es(data):\n",
    "    corpus = []\n",
    "    for tweets in data:\n",
    "        tweets_lowered = tweets.lower()\n",
    "\n",
    "        # Further tweet sanitation\n",
    "        tweet = re.sub(r'\\s+[a-z]\\s+', ' ', tweets_lowered) # remove single characters like i and a\n",
    "        tweet = re.sub(r'^[a-z]\\s+', ' ', tweet)  # remove single characters at the beginning like i and a\n",
    "        tweet = re.sub(r'\\s+', ' ', tweet)  # remove extra spaces\n",
    "\n",
    "        #tweets_tokenized = word_tokenize(tweet)\n",
    "        #tweets_tokenized = word_tokenize(tweets_lowered)\n",
    "        #tweets_corpus = [w for w in tweets_tokenized if w not in stop_words_es]\n",
    "\n",
    "        #corpus.append(tweets_corpus)\n",
    "\n",
    "        # lemmatize words\n",
    "        tokenizedTweet = nlpES(tweet)\n",
    "        # Sentiment analyzer\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        score = analyser.polarity_scores(tweet)\n",
    "        # Convert dictionary into string\n",
    "        score = str(score) \n",
    "        processedTweet = []\n",
    "        for l in tokenizedTweet:\n",
    "           # processedTweet.append(f\"{l.lemma_}\")\n",
    "            processedTweet.append(f\"{l.lemma_}({l.pos_})\")\n",
    "        processedTweet.append(score)\n",
    "        corpus.append(processedTweet)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the English and Spanish corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# English \n",
    "processed_train_split_corpus_en = en_training_div_data\n",
    "processed_train_split_corpus_en['text'] = build_tokenized_corpus_en(en_training_div_data['text'])\n",
    "processed_test_split_corpus_en = en_testing_div_data\n",
    "processed_test_split_corpus_en['text'] = build_tokenized_corpus_en(en_testing_div_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish bots\n",
    "processed_train_split_corpus_es = es_training_div_data\n",
    "processed_train_split_corpus_es['text'] = build_tokenized_corpus_es(es_training_div_data['text'])\n",
    "processed_test_split_corpus_es = es_testing_div_data\n",
    "processed_test_split_corpus_es['text'] = build_tokenized_corpus_es(es_testing_div_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most common words using TF-IDF gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_en(processed_train_split_corpus):\n",
    "    # Create a gensim dictionary\n",
    "    dictionary = Dictionary(processed_train_split_corpus)\n",
    "    # Turn the tweets (docs) into BOW\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_train_split_corpus]\n",
    "    unwanted_most_common_words = {}\n",
    "    # Create the defaultdict: total_word_count\n",
    "    total_word_count = defaultdict(int)\n",
    "    for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "        total_word_count[word_id] += word_count\n",
    "\n",
    "    # Create a sorted list from the defaultdict: sorted_word_count \n",
    "    sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "    # Find the top 100 words across all documents alongside the count\n",
    "    for word_id, word_count in sorted_word_count[:100]:\n",
    "        unwanted_most_common_words[dictionary.get(word_id)] = word_count\n",
    "\n",
    "    # We need to keep track of the special tags we created in the preprocessing\n",
    "    # so we just popped them from the 100 list\n",
    "    #unwanted_most_common_words.pop(\"URL\")\n",
    "    #unwanted_most_common_words.pop(\"MENTION\")\n",
    "    #unwanted_most_common_words.pop(\"HASHTAG\")\n",
    "    #unwanted_most_common_words.pop(\"QUOTE\")\n",
    "    #unwanted_most_common_words.pop(\"rt\")\n",
    "    \n",
    "    return list(unwanted_most_common_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_es(processed_train_split_corpus):\n",
    "    # Create a gensim dictionary\n",
    "    dictionary = Dictionary(processed_train_split_corpus)\n",
    "    # Turn the tweets (docs) into BOW\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_train_split_corpus]\n",
    "    unwanted_most_common_words = {}\n",
    "    # Create the defaultdict: total_word_count\n",
    "    total_word_count = defaultdict(int)\n",
    "    for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "        total_word_count[word_id] += word_count\n",
    "\n",
    "    # Create a sorted list from the defaultdict: sorted_word_count \n",
    "    sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "    # For Spanish, using only the top 100 words worked best\n",
    "    for word_id, word_count in sorted_word_count[:100]:\n",
    "        unwanted_most_common_words[dictionary.get(word_id)] = word_count\n",
    "        \n",
    "    #unwanted_most_common_words.pop(\"URL(PROPN)\")\n",
    "    #unwanted_most_common_words.pop(\"MENTION(PROPN)\")\n",
    "    #unwanted_most_common_words.pop(\"HASHTAG(PROPN)\")\n",
    "    #unwanted_most_common_words.pop(\"QUOTE(PROPN)\")\n",
    "    #unwanted_most_common_words.pop(\"rt(CCONJ)\")\n",
    "    \n",
    "    return list(unwanted_most_common_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "unwanted_en = find_most_common_en(processed_train_split_corpus_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish\n",
    "unwanted_es = find_most_common_es(processed_train_split_corpus_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the lists of tokens back to tweet_bodies while deleting all unwanted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tokens_into_tweets(data):\n",
    "    #corpus = []\n",
    "    for i in range(0, len(data)):\n",
    "        #tweet = data[i]\n",
    "        #tweet = ' '.join([w for w in tweet])\n",
    "        tweet = [' '.join(t) for t in data]\n",
    "     #   corpus.append(tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_words_from_tweets(data, unwanted):\n",
    "    corpus = []\n",
    "    for i in range(0, len(data)):\n",
    "        tweet = data[i].split()\n",
    "        tweet = ' '.join([w for w in tweet if w not in unwanted])\n",
    "        corpus.append(tweet)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "combined_training_en = processed_train_split_corpus_en\n",
    "combined_training_en['text'] = combine_tokens_into_tweets(processed_train_split_corpus_en['text'])\n",
    "combined_testing_en = processed_test_split_corpus_en\n",
    "combined_testing_en['text'] = combine_tokens_into_tweets(processed_test_split_corpus_en['text'])\n",
    "#processed_training_corpus_en = remove_words_from_tweets(combined_training_en, unwanted_en)\n",
    "#processed_testing_corpus_en = remove_words_from_tweets(combined_testing_en, unwanted_en)\n",
    "processed_training_corpus_en = combined_training_en\n",
    "processed_testing_corpus_en = combined_testing_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish\n",
    "combined_training_es = processed_train_split_corpus_es\n",
    "combined_training_es['text'] = combine_tokens_into_tweets(processed_train_split_corpus_es['text'])\n",
    "combined_testing_es = processed_test_split_corpus_es\n",
    "combined_testing_es['text'] = combine_tokens_into_tweets(processed_test_split_corpus_es['text'])\n",
    "#processed_training_corpus_es = remove_words_from_tweets(combined_training_es, unwanted_es)\n",
    "#processed_testing_corpus_es = remove_words_from_tweets(combined_testing_es, unwanted_es)\n",
    "processed_training_corpus_es = combined_training_es\n",
    "processed_testing_corpus_es = combined_testing_es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English\n",
    "training_corpus_labels_en = en_training_div_data['author']\n",
    "testing_corpus_labels_en = en_testing_div_data['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanish\n",
    "training_corpus_labels_es = es_training_div_data['author']\n",
    "testing_corpus_labels_es = es_testing_div_data['author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds after preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('English non hate spreaders common words:')\n",
    "subset=processed_training_corpus_en[processed_training_corpus_en.author==0]\n",
    "text=subset.text.values\n",
    "words =\" \".join(text)\n",
    "create_wordcloud(words)\n",
    "\n",
    "print('English hate spreaders common words:')\n",
    "subset=processed_training_corpus_en[processed_training_corpus_en.author==1]\n",
    "text=subset.text.values\n",
    "words =\" \".join(text)\n",
    "create_wordcloud(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spanish non hate spreaders common words:')\n",
    "subset=processed_training_corpus_es[processed_training_corpus_es.author==0]\n",
    "text=subset.text.values\n",
    "words =\" \".join(text)\n",
    "create_wordcloud(words)\n",
    "\n",
    "print('Spanish hate spreaders common words:')\n",
    "subset=processed_training_corpus_es[processed_training_corpus_es.author==1]\n",
    "text=subset.text.values\n",
    "words =\" \".join(text)\n",
    "create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "vectorizer_en = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,5), max_features=500, stop_words=stop_words_en)\n",
    "vectorized_en_training_div_tweets = vectorizer_en.fit_transform(processed_training_corpus_en['text']).toarray()\n",
    "vectorized_en_testing_div_tweets = vectorizer_en.transform(processed_testing_corpus_en['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish\n",
    "vectorizer_es = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,5), max_features=500, stop_words=stop_words_es)\n",
    "vectorized_es_training_div_tweets = vectorizer_es.fit_transform(processed_training_corpus_es['text']).toarray()\n",
    "vectorized_es_testing_div_tweets = vectorizer_es.transform(processed_testing_corpus_es['text']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: ML Hyperparameter validation testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classifier testing (no hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "print('English baselines')\n",
    "en_ML_model = SVC()\n",
    "en_ML_model.fit(vectorized_en_training_div_tweets, training_corpus_labels_en)\n",
    "print(\"SVM:\" , accuracy_score(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "\n",
    "\n",
    "en_ML_model = ensemble.RandomForestClassifier()\n",
    "en_ML_model.fit(vectorized_en_training_div_tweets, training_corpus_labels_en)\n",
    "print(\"Random forest:\" , accuracy_score(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "\n",
    "en_ML_model = DecisionTreeClassifier()\n",
    "en_ML_model.fit(vectorized_en_training_div_tweets, training_corpus_labels_en)\n",
    "print(\"Decision Tree:\" , accuracy_score(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish\n",
    "print('Spanish baselines')\n",
    "es_ML_model = SVC(C=20, kernel='linear')\n",
    "es_ML_model.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "print(\"SVM:\" , accuracy_score(es_ML_model.predict(vectorized_es_testing_div_tweets), testing_corpus_labels_es))\n",
    "\n",
    "es_ML_model = ensemble.RandomForestClassifier(n_estimators= 200)\n",
    "es_ML_model.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "print(\"Random forest:\" , accuracy_score(es_ML_model.predict(vectorized_es_testing_div_tweets), testing_corpus_labels_es))\n",
    "\n",
    "es_ML_model = DecisionTreeClassifier(criterion='gini')\n",
    "es_ML_model.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "print(\"Decision Tree:\" , accuracy_score(es_ML_model.predict(vectorized_es_testing_div_tweets), testing_corpus_labels_es))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a parameter dictionary to apply gridsearchCV on to find the best model for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto'),\n",
    "        'params' : {\n",
    "            'C': [1,10,20],\n",
    "            'kernel': ['rbf','linear']\n",
    "        }  \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': ensemble.RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators': [1,5,10, 50, 100, 200, 400, 500, 700]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1,5,10, 50, 100]\n",
    "        }\n",
    "    },\n",
    "    'naive_bayes_gaussian': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'naive_bayes_multinomial': {\n",
    "        'model': MultinomialNB(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'criterion': ['gini','entropy'],\n",
    "        }\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform gridsearchCV on the parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "evaluation_en = []\n",
    "for model_name, mp in model_params.items():\n",
    "        clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False )\n",
    "        clf.fit(vectorized_en_training_div_tweets,training_corpus_labels_en)\n",
    "        evaluation_en.append({\n",
    "            'model': model_name,\n",
    "            'best_score': clf.best_score_,\n",
    "            'best_params': clf.best_params_\n",
    "        })\n",
    "df = pd.DataFrame(evaluation_en, columns=['model', 'best_score', 'best_params'])\n",
    "print('Best English parameters')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish\n",
    "evaluation_es = []\n",
    "for model_name, mp in model_params.items():\n",
    "        clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False )\n",
    "        clf.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "        evaluation_es.append({\n",
    "            'model': model_name,\n",
    "            'best_score': clf.best_score_,\n",
    "            'best_params': clf.best_params_\n",
    "        })\n",
    "df = pd.DataFrame(evaluation_es, columns=['model', 'best_score', 'best_params'])\n",
    "print('Best Spanish parameters')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the best parameters (must change parameters based on gridsearch results manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English \n",
    "print ('English validation set accuracies')\n",
    "\n",
    "en_ML_model = SVC(C=1, kernel='linear')\n",
    "en_ML_model.fit(vectorized_en_training_div_tweets,training_corpus_labels_en)\n",
    "print(\"SVM:\" , accuracy_score(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "# print(classification_report(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "en_ML_model = ensemble.RandomForestClassifier(n_estimators=10)\n",
    "en_ML_model.fit(vectorized_en_training_div_tweets,training_corpus_labels_en)\n",
    "print(\"Random forest:\" , accuracy_score(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "# print(classification_report(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "en_ML_model = GaussianNB()\n",
    "en_ML_model.fit(vectorized_en_training_div_tweets,training_corpus_labels_en)\n",
    "print(\"Naive Bayes G:\" , accuracy_score(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "# print(classification_report(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "en_ML_model = MultinomialNB()\n",
    "en_ML_model.fit(vectorized_en_training_div_tweets,training_corpus_labels_en)\n",
    "print(\"Naive Bayes MN:\" , accuracy_score(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "# print(classification_report(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "en_ML_model = DecisionTreeClassifier(criterion='gini')\n",
    "en_ML_model.fit(vectorized_en_training_div_tweets,training_corpus_labels_en)\n",
    "print(\"Decision Tree:\" , accuracy_score(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))\n",
    "# print(classification_report(en_ML_model.predict(vectorized_en_testing_div_tweets),testing_corpus_labels_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish \n",
    "print('Spanish bot task validation set accuracies')\n",
    "\n",
    "es_ML_model = SVC(C=1, kernel='linear')\n",
    "es_ML_model.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "print(\"SVM:\" , accuracy_score(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "# print(classification_report(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "es_ML_model = ensemble.RandomForestClassifier(n_estimators=400)\n",
    "es_ML_model.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "print(\"Random forest:\" , accuracy_score(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "# print(classification_report(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "es_ML_model = GaussianNB()\n",
    "es_ML_model.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "print(\"Naive Bayes G:\" , accuracy_score(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "# print(classification_report(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "es_ML_model = MultinomialNB()\n",
    "es_ML_model.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "print(\"Naive Bayes MN:\" , accuracy_score(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "# print(classification_report(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "es_ML_model = DecisionTreeClassifier(criterion='entropy')\n",
    "es_ML_model.fit(vectorized_es_training_div_tweets,training_corpus_labels_es)\n",
    "print(\"Decision Tree:\" , accuracy_score(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))\n",
    "# print(classification_report(es_ML_model.predict(vectorized_es_testing_div_tweets),testing_corpus_labels_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do:\n",
    "    # Try different preprocessing steps\n",
    "    # Try removing most common words\n",
    "    # Add list of Hate Speech terms (look for corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
